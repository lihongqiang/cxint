{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过course_union_synset文件，补充同义词表和描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SynID与Synset转换函数\n",
    "from nltk.corpus import wordnet\n",
    "def getSynset(wnid):\n",
    "    pos = wnid[0]\n",
    "    offset = wnid[1:]\n",
    "    return wordnet._synset_from_pos_and_offset(pos,int(offset))\n",
    "def getWordnetID(syn):\n",
    "    return syn.pos()+('%08d' % syn.offset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#读取course_union_synset文件，生成synset的所有同义词表\n",
    "synsetID_list = open(\"course_union_synid.txt\")\n",
    "synsets_words_list = open(\"course_synsets_words.txt\", \"wb\")\n",
    "for line in synsetID_list:\n",
    "    synid = line.strip('\\n')\n",
    "    syn = getSynset(synid)\n",
    "    lemma_names = [str(lemma) for lemma in syn.lemma_names()]\n",
    "    synsets_words_list.write(synid + '\\t')\n",
    "    for index, word in enumerate(lemma_names):\n",
    "        if index == len(lemma_names)-1:\n",
    "            synsets_words_list.write(word + '\\n')\n",
    "        else:\n",
    "            synsets_words_list.write(word + '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SearchResult:\n",
    "    def __init__(self):\n",
    "        self.url= '' \n",
    "        self.title = '' \n",
    "        self.content = '' \n",
    "\n",
    "    def getURL(self):\n",
    "        return self.url\n",
    "\n",
    "    def setURL(self, url):\n",
    "        self.url = url \n",
    "\n",
    "    def getTitle(self):\n",
    "        return self.title\n",
    "\n",
    "    def setTitle(self, title):\n",
    "        self.title = title\n",
    "\n",
    "    def getContent(self):\n",
    "        return self.content\n",
    "\n",
    "    def setContent(self, content):\n",
    "        self.content = content\n",
    "\n",
    "    def printIt(self, prefix = ''):\n",
    "        print 'url\\t->', self.url\n",
    "        print 'title\\t->', self.title\n",
    "        print 'content\\t->', self.content\n",
    "        print \n",
    "\n",
    "    def writeFile(self, filename):\n",
    "        file = open(filename, 'a')\n",
    "        try:\n",
    "            file.write('url:' + self.url+ '\\n')\n",
    "            file.write('title:' + self.title + '\\n')\n",
    "            file.write('content:' + self.content + '\\n\\n')\n",
    "\n",
    "        except IOError, e:\n",
    "            print 'file error:', e\n",
    "        finally:\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract serach results list from downloaded html file\n",
    "def extractSearchResults(html):\n",
    "    results = list()\n",
    "    soup = BeautifulSoup(html)\n",
    "    div = soup.find('div', id  = 'search')\n",
    "    if (type(div) != types.NoneType):\n",
    "        lis = div.findAll('li', {'class': 'g'})\n",
    "        if(len(lis) > 0):\n",
    "            for li in lis:\n",
    "                result = SearchResult()\n",
    "                h3 = li.find('h3', {'class': 'r'})\n",
    "                if(type(h3) == types.NoneType):\n",
    "                    continue\n",
    "\n",
    "                # extract domain and title from h3 object\n",
    "                link = h3.find('a')\n",
    "                if (type(link) == types.NoneType):\n",
    "                    continue\n",
    "\n",
    "                url = link['href']\n",
    "                url = self.extractUrl(url)\n",
    "                if(cmp(url, '') == 0):\n",
    "                    continue\n",
    "                title = link.renderContents()\n",
    "                result.setURL(url)\n",
    "                result.setTitle(title)\n",
    "\n",
    "                span = li.find('span', {'class': 'st'})\n",
    "                if (type(span) != types.NoneType):\n",
    "                    content = span.renderContents()\n",
    "                    result.setContent(content)\n",
    "                results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import urllib2, socket, time\n",
    "import gzip, StringIO\n",
    "import re, random, types\n",
    "\n",
    "def query(name):\n",
    "    url = 'https://www.google.com.hk/search'\n",
    "    proxies = { \"http\": \"http://10.18.105.200:3128\", \"https\": \"http://10.18.105.200:3128\", } \n",
    "    params = {\n",
    "        'hl': 'en',\n",
    "        'num': 100,\n",
    "        'start': 0,\n",
    "        'q': name,\n",
    "        'safe': 'strict',\n",
    "        'biw': 2560,\n",
    "        'bih': 1302,\n",
    "        'source': 'lnms',\n",
    "        'tbm': 'isch',\n",
    "        'sa': 'X',\n",
    "        'ved': '0ahUKEwi0qK2WmtfRAhXBFZQKHdedAoEQ_AUIBigB',\n",
    "    }\n",
    "    content = requests.get(url, proxies=proxies, params=params, timeout=1).content\n",
    "    results = extractSearchResults(content)\n",
    "    print results\n",
    "#     soup = BeautifulSoup(content, 'html.parser')\n",
    "#     print soup.prettify()\n",
    "query('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "import urllib2, socket, time\n",
    "import gzip, StringIO\n",
    "import re, random, types\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "base_url = 'https://www.google.com.hk/'\n",
    "results_per_page = 10\n",
    "\n",
    "user_agents = list()\n",
    "\n",
    "# results from the search engine\n",
    "# basically include url, title,content\n",
    "class SearchResult:\n",
    "    def __init__(self):\n",
    "        self.url= '' \n",
    "        self.title = '' \n",
    "        self.content = '' \n",
    "\n",
    "    def getURL(self):\n",
    "        return self.url\n",
    "\n",
    "    def setURL(self, url):\n",
    "        self.url = url \n",
    "\n",
    "    def getTitle(self):\n",
    "        return self.title\n",
    "\n",
    "    def setTitle(self, title):\n",
    "        self.title = title\n",
    "\n",
    "    def getContent(self):\n",
    "        return self.content\n",
    "\n",
    "    def setContent(self, content):\n",
    "        self.content = content\n",
    "\n",
    "    def printIt(self, prefix = ''):\n",
    "        print 'url\\t->', self.url\n",
    "        print 'title\\t->', self.title\n",
    "        print 'content\\t->', self.content\n",
    "        print \n",
    "\n",
    "    def writeFile(self, filename):\n",
    "        file = open(filename, 'a')\n",
    "        try:\n",
    "            file.write('url:' + self.url+ '\\n')\n",
    "            file.write('title:' + self.title + '\\n')\n",
    "            file.write('content:' + self.content + '\\n\\n')\n",
    "\n",
    "        except IOError, e:\n",
    "            print 'file error:', e\n",
    "        finally:\n",
    "            file.close()\n",
    "\n",
    "\n",
    "class GoogleAPI:\n",
    "    def __init__(self):\n",
    "        timeout = 40\n",
    "        socket.setdefaulttimeout(timeout)\n",
    "\n",
    "    def randomSleep(self):\n",
    "        sleeptime =  random.randint(60, 120)\n",
    "        time.sleep(sleeptime)\n",
    "\n",
    "    #extract the domain of a url\n",
    "    def extractDomain(self, url):\n",
    "        domain = ''\n",
    "        pattern = re.compile(r'http[s]?://([^/]+)/', re.U | re.M)\n",
    "        url_match = pattern.search(url)\n",
    "        if(url_match and url_match.lastindex > 0):\n",
    "            domain = url_match.group(1)\n",
    "\n",
    "        return domain\n",
    "\n",
    "    #extract a url from a link\n",
    "    def extractUrl(self, href):\n",
    "        url = ''\n",
    "        pattern = re.compile(r'(http[s]?://[^&]+)&', re.U | re.M)\n",
    "        url_match = pattern.search(href)\n",
    "        if(url_match and url_match.lastindex > 0):\n",
    "            url = url_match.group(1)\n",
    "\n",
    "        return url \n",
    "\n",
    "    # extract serach results list from downloaded html file\n",
    "    def extractSearchResults(self, html):\n",
    "        results = list()\n",
    "        soup = BeautifulSoup(html)\n",
    "        div = soup.find('div', id  = 'search')\n",
    "        if (type(div) != types.NoneType):\n",
    "            lis = div.findAll('li', {'class': 'g'})\n",
    "            if(len(lis) > 0):\n",
    "                for li in lis:\n",
    "                    result = SearchResult()\n",
    "                    h3 = li.find('h3', {'class': 'r'})\n",
    "                    if(type(h3) == types.NoneType):\n",
    "                        continue\n",
    "\n",
    "                    # extract domain and title from h3 object\n",
    "                    link = h3.find('a')\n",
    "                    if (type(link) == types.NoneType):\n",
    "                        continue\n",
    "\n",
    "                    url = link['href']\n",
    "                    url = self.extractUrl(url)\n",
    "                    if(cmp(url, '') == 0):\n",
    "                        continue\n",
    "                    title = link.renderContents()\n",
    "                    result.setURL(url)\n",
    "                    result.setTitle(title)\n",
    "\n",
    "                    span = li.find('span', {'class': 'st'})\n",
    "                    if (type(span) != types.NoneType):\n",
    "                        content = span.renderContents()\n",
    "                        result.setContent(content)\n",
    "                    results.append(result)\n",
    "        return results\n",
    "\n",
    "    # search web\n",
    "    # @param query -> query key words \n",
    "    # @param lang -> language of search results  \n",
    "    # @param num -> number of search results to return \n",
    "    def search(self, query, lang='en', num=results_per_page):\n",
    "\n",
    "        # 设置代理\n",
    "        proxy=urllib2.ProxyHandler({'https': 'http://10.18.105.200:3128'})\n",
    "        opener=urllib2.build_opener(proxy)\n",
    "        urllib2.install_opener(opener)\n",
    "\n",
    "        proxy=urllib2.ProxyHandler({'http': 'http://10.18.105.200:3128'})\n",
    "        opener=urllib2.build_opener(proxy)\n",
    "        urllib2.install_opener(opener)\n",
    "\n",
    "        search_results = list()\n",
    "        query = urllib2.quote(query)\n",
    "        if(num % results_per_page == 0):\n",
    "            pages = num / results_per_page\n",
    "        else:\n",
    "            pages = num / results_per_page + 1\n",
    "\n",
    "        for p in range(0, pages):\n",
    "            start = p * results_per_page \n",
    "            url = '%s/search?hl=%s&num=%d&start=%s&q=%s' % (base_url, lang, results_per_page, start, query)\n",
    "            retry = 3\n",
    "            while(retry > 0):\n",
    "                try:\n",
    "                    request = urllib2.Request(url)\n",
    "                    length = len(user_agents)\n",
    "                    index = random.randint(0, length-1)\n",
    "                    user_agent = user_agents[index] \n",
    "                    request.add_header('User-agent', user_agent)\n",
    "                    request.add_header('connection','keep-alive')\n",
    "                    request.add_header('Accept-Encoding', 'gzip')\n",
    "                    request.add_header('referer', base_url)\n",
    "                    response = urllib2.urlopen(request)\n",
    "                    html = response.read() \n",
    "                    if(response.headers.get('content-encoding', None) == 'gzip'):\n",
    "                        html = gzip.GzipFile(fileobj=StringIO.StringIO(html)).read()\n",
    "\n",
    "                    results = self.extractSearchResults(html)\n",
    "                    search_results.extend(results)\n",
    "                    break;\n",
    "                except urllib2.URLError,e:\n",
    "                    print 'url error:', e\n",
    "                    self.randomSleep()\n",
    "                    retry = retry - 1\n",
    "                    continue\n",
    "                \n",
    "                except Exception, e:\n",
    "                    print 'error:', e\n",
    "                    retry = retry - 1\n",
    "                    self.randomSleep()\n",
    "                    continue\n",
    "        return search_results \n",
    "\n",
    "def load_user_agent():\n",
    "    fp = open('./user_agents', 'r')\n",
    "\n",
    "    line  = fp.readline().strip('\\n')\n",
    "    while(line):\n",
    "        user_agents.append(line)\n",
    "        line = fp.readline().strip('\\n')\n",
    "    fp.close()\n",
    "\n",
    "def crawler():\n",
    "    # Load use agent string from file\n",
    "    load_user_agent()\n",
    "\n",
    "    # Create a GoogleAPI instance\n",
    "    api = GoogleAPI()\n",
    "\n",
    "    # set expect search results to be crawled\n",
    "    expect_num = 10\n",
    "    # if no parameters, read query keywords from file\n",
    "    if(len(sys.argv) < 2):\n",
    "        keywords = open('./keywords', 'r')\n",
    "        keyword = keywords.readline()\n",
    "        while(keyword):\n",
    "            results = api.search(keyword, num = expect_num)\n",
    "            for r in results:\n",
    "                r.printIt()\n",
    "            keyword = keywords.readline()\n",
    "        keywords.close()\n",
    "    else:\n",
    "        keyword = sys.argv[1]\n",
    "        results = api.search(keyword, num = expect_num)\n",
    "        for r in results:\n",
    "            r.printIt()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4735c869dcc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-3947d494ee9d>\u001b[0m in \u001b[0;36mcrawler\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mkeyword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpect_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintIt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3947d494ee9d>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, query, lang, num)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0;32mprint\u001b[0m \u001b[0;34m'url error:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                     \u001b[0mretry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3947d494ee9d>\u001b[0m in \u001b[0;36mrandomSleep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrandomSleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0msleeptime\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleeptime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#extract the domain of a url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f510c9e33fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.google.com.hk/search?q=cat&safe=strict&biw=2560&bih=1302&source=lnms&tbm=isch&sa=X&ved=0ahUKEwi0qK2WmtfRAhXBFZQKHdedAoEQ_AUIBigB#imgrc=jvL3N0bysq2MmM%3A'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lihongqiang/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lihongqiang/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lihongqiang/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 548\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lihongqiang/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lihongqiang/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lihongqiang/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_full_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "#coding=utf8\n",
    "\n",
    "import urllib2\n",
    "\n",
    "proxy = urllib2.ProxyHandler({'http':'http://10.18.105.200:3128'})\n",
    "opener = urllib2.build_opener(proxy)\n",
    "urllib2.install_opener(opener)\n",
    "\n",
    "proxy = urllib2.ProxyHandler({'https':'http://10.18.105.200:3128'})\n",
    "opener = urllib2.build_opener(proxy)\n",
    "urllib2.install_opener(opener)\n",
    "\n",
    "response = urllib2.urlopen('https://www.google.com.hk/search?q=cat&safe=strict&biw=2560&bih=1302&source=lnms&tbm=isch&sa=X&ved=0ahUKEwi0qK2WmtfRAhXBFZQKHdedAoEQ_AUIBigB#imgrc=jvL3N0bysq2MmM%3A')\n",
    "\n",
    "print response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
