{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n",
      "n2084071\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need more than 1 value to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8bc8ea54015f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/lihongqiang/anaconda2/lib/python2.7/site-packages/nltk/corpus/reader/wordnet.pyc\u001b[0m in \u001b[0;36msynset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;31m# split name into lemma, part of speech and synset number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset_index_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0msynset_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset_index_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need more than 1 value to unpack"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "print wn.synsets('dog')\n",
    "dog = wn.synsets('dog')[0]\n",
    "print str(dog.pos())+str(dog.offset())\n",
    "off = dog.offset()\n",
    "print wn.synset(str(off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取machine_train_label文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "# machine_train_label = codecs.open(\"./data/labels.csv\", \"r\", \"utf-8\")\n",
    "machine_train_label = open(\"./data/labels.csv\", \"r\")\n",
    "label_dict = {}\n",
    "for line in machine_train_label:\n",
    "    ImageID,Source,LabelName,Confidence = line.split(',')\n",
    "    if label_dict.has_key(LabelName):\n",
    "        label_dict[LabelName] += 1\n",
    "    else:\n",
    "        label_dict[LabelName] = 1;\n",
    "machine_train_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取machine_valid_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "machine_valid_label = codecs.open(\"./data/labels_valid.csv\", \"r\", \"utf-8\")\n",
    "for line in machine_valid_label:\n",
    "    ImageID,Source,LabelName,Confidence = line.split(',')\n",
    "    if label_dict.has_key(LabelName):\n",
    "        label_dict[LabelName] += 1\n",
    "    else:\n",
    "        label_dict[LabelName] = 1;\n",
    "machine_valid_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取humman_valid_label文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "humman_valid_label = codecs.open(\"./data/labels_humman_valid.csv\", \"r\", \"utf-8\")\n",
    "for line in humman_valid_label:\n",
    "    ImageID,Source,LabelName,Confidence = line.split(',')\n",
    "    if label_dict.has_key(LabelName):\n",
    "        label_dict[LabelName] += 1\n",
    "    else:\n",
    "        label_dict[LabelName] = 1;\n",
    "humman_valid_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出频数前2000的LabelName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top3000_label = sorted(label_dict.iteritems(), key = lambda d:d[1], reverse=True)\n",
    "for label in top3000_label:\n",
    "    print (label[0] + '\\t' + str(label[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示前2000标签的分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top3000_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7c6f9b31e661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop3000_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop3000_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'num = '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop3000_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop3000_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top3000_label' is not defined"
     ]
    }
   ],
   "source": [
    "top3000_label = top3000_label[:2000]\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(1)\n",
    "print 'num = '+ str(len(top3000_label))\n",
    "x = [num for num in range(len(top3000_label))]\n",
    "y = [label[1] for label in top3000_label]\n",
    "plt.xlabel('LabelName')\n",
    "plt.ylabel('Frequency')\n",
    "plt.plot(x, y)\n",
    "#plt.xlim(0, 2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "label_file = codecs.open(u'./data/labels_file.tsv', \"a\", \"utf-8\")\n",
    "num = 0\n",
    "for label in top3000_label:\n",
    "    label_file.write(label[0] + '\\t' + str(label[1]) + '\\n')\n",
    "    num += 1\n",
    "label_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取dict文件中所有的label名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_file = codecs.open(u'./data/dict.csv', \"r\")\n",
    "label_dict = {}\n",
    "n=0\n",
    "for line in dict_file:\n",
    "    print line\n",
    "    try:\n",
    "        label_id, label_name = line.split(\",\", 1)\n",
    "        label_id = label_id.strip('\\t\\r\\n\\\"')\n",
    "        label_name = label_name.strip('\\t\\r\\n\\\"')\n",
    "#         print label_id, label_name\n",
    "        label_dict[label_id] = label_name\n",
    "    except:\n",
    "        print n\n",
    "        print line + ' error'\n",
    "    n+=1\n",
    "dict_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_match_file = codecs.open(u'./data/labels_match_file.tsv', \"a\", \"utf-8\")\n",
    "print len(top3000_label)\n",
    "for label in top3000_label:\n",
    "    try:\n",
    "#         print (label[0] + '\\t' + str(label[1]) + '\\t' + label_dict[label[0]] + '\\n')\n",
    "        label_match_file.write(label[0] + '\\t' + str(label[1]) + '\\t' + label_dict[label[0]] + '\\n')\n",
    "    except:\n",
    "        print (label[0] + '\\t' + str(label[1]) + '\\t' + label_dict[label[0]]) \n",
    "label_match_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取WordNet中的所有Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordNet_dict = codecs.open(\"./data/index.tab\", \"r\", \"utf-8\")\n",
    "word_dict = {}\n",
    "for word in wordNet_dict:\n",
    "    word = word.split('\\t')\n",
    "    word[0] = word[0].strip('\\t\\n\\r')\n",
    "    word[1] = word[1].strip('\\t\\n\\r')\n",
    "    word_dict[word[0]] = word[1]\n",
    "wordNet_dict.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取WordNet中的所有Sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.匹配openimage的前2000的label和wordnet中的word,通过wordnet3.0中的standoff的term数据进行判断是否在wordnet中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordnet_openimage_label = codecs.open(\"./data/wordnet_openimage_label.tsv\", \"a\", \"utf-8\")\n",
    "num_in = 0\n",
    "num_out = 0\n",
    "num_error = 0\n",
    "for label in top3000_label:\n",
    "    label_name_origin = label_dict[label[0]]\n",
    "    label_name = '_'.join(label_name_origin.split(' '))\n",
    "    try:\n",
    "        if label_name in word_dict:\n",
    "            wordnet_openimage_label.write(label[0] + '\\t' + str(label[1]) + '\\t' + label_name + '\\t'+ word_dict[label_name] + '\\n')\n",
    "            num_in += 1\n",
    "            #print label_name + '\\t' + word_dict[label_name]\n",
    "        else:\n",
    "            #去掉复数形式之后在WordNet中\n",
    "            if label_name[:-1] in word_dict:\n",
    "                label_name = label_name[:-1]\n",
    "                wordnet_openimage_label.write(label[0] + '\\t' + str(label[1]) + '\\t' + label_name + '\\t'+ word_dict[label_name] + '\\n')\n",
    "                num_in += 1\n",
    "                continue\n",
    "            \n",
    "            #空格隔开的某一个词在WordNet中\n",
    "            flag = False\n",
    "            for word in label_name_origin.split(' '):\n",
    "                if word in word_dict:\n",
    "                    label_name = word\n",
    "                    wordnet_openimage_label.write(label[0] + '\\t' + str(label[1]) + '\\t' + label_name + '\\t'+ word_dict[label_name] + '\\n')\n",
    "                    num_in += 1\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            \n",
    "            #不在WordNet内\n",
    "            num_out += 1\n",
    "            wordnet_openimage_label.write(label[0] + '\\t' + str(label[1]) + '\\t' + label_name + '\\t' + '' + '\\n')\n",
    "            #print label_name + '\\t' + 'No'\n",
    "    except:\n",
    "        print label_name + ' error'\n",
    "        num_error += 1\n",
    "wordnet_openimage_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.匹配openimage的前2000的label和wordnet中的word,使用nltk中的wordnet工具包来判断label是否在wordnet中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wordnet.synsets('toy_dog')\n",
    "print \n",
    "dog = wordnet.synset('dog.n.01')\n",
    "print dog.unicode_repr()\n",
    "print dog.name()\n",
    "print dog.offset()\n",
    "print dog.pos()\n",
    "print dog.lemmas()\n",
    "print dog.lemma_names()\n",
    "print dog.lexname()\n",
    "print dog.definition()\n",
    "print dog.examples()\n",
    "print wordnet.ss2of(dog)\n",
    "print wordnet.of2ss(wordnet.ss2of(dog))\n",
    "\n",
    "print\n",
    "dog = wordnet.synset('dog.n.03')\n",
    "print dog.unicode_repr()\n",
    "print dog.name()\n",
    "print dog.offset()\n",
    "print dog.pos()\n",
    "print dog.lemmas()\n",
    "print dog.lemma_names()\n",
    "print dog.lexname()\n",
    "print dog.definition()\n",
    "print dog.examples()\n",
    "\n",
    "print \n",
    "hyp = lambda s:s.hypernyms()\n",
    "print list(dog.closure(hyp))\n",
    "\n",
    "print\n",
    "from pprint import pprint\n",
    "pprint(dog.tree(hyp))\n",
    "\n",
    "print wordnet.morphy('dogs')\n",
    "of = (dog.offset())\n",
    "print '%09d' % of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordnet_openimage_description = codecs.open(\"./data/wordnet_openimage_description.tsv\", \"a\", \"utf-8\")\n",
    "des_dict = {}\n",
    "\n",
    "def getlabelSyn(syns):\n",
    "    label_syn = []\n",
    "    for syn in syns:\n",
    "        wnid = syn.pos() + '%08d' % syn.offset()\n",
    "        if wnid not in des_dict:\n",
    "            lex = syn.lexname()\n",
    "            defin = syn.definition()\n",
    "            des_dict[wnid] = lex + '\\t' + defin\n",
    "#         label_syn.append(wnid+'||'+lex+'||'+defin)\n",
    "        label_syn.append(wnid)\n",
    "    return label_syn\n",
    "\n",
    "wordnet_openimage_label = codecs.open(\"./data/wordnet_openimage_label.tsv\", \"a\", \"utf-8\")\n",
    "num_in = 0\n",
    "num_out = 0\n",
    "num_error = 0\n",
    "for label in top3000_label:\n",
    "    label_name_origin = label_dict[label[0]]\n",
    "    label_name = '_'.join(label_name_origin.split(' ')) \n",
    "    try:\n",
    "        syns = wordnet.synsets(label_name)\n",
    "        if len(syns):\n",
    "            label_syn = getlabelSyn(syns)\n",
    "            wordnet_openimage_label.write(label[0] + '\\t' + str(label[1]) + '\\t' + label_name_origin + '\\t'+ ','.join(label_syn) + '\\n')\n",
    "#             print label[0] + '\\t' + str(label[1]) + '\\t' + label_name + '\\t'+ '\\t'.join(label_syn)\n",
    "            num_in += 1\n",
    "\n",
    "        else:       \n",
    "            #空格隔开的某一个词在WordNet中\n",
    "            flag = False\n",
    "            label_synsets = []\n",
    "            for word in label_name_origin.split(' '):\n",
    "                if len(wordnet.synsets(word)):\n",
    "                    syns = wordnet.synsets(word)\n",
    "                    label_syn = getlabelSyn(syns)\n",
    "                    label_synsets.extend(label_syn)\n",
    "                    flag = True\n",
    "            if flag:\n",
    "                num_in += 1\n",
    "#                 print label[0] + '\\t' + str(label[1]) + '\\t' + label_name + '\\t'+ '\\t'.join(label_synsets)\n",
    "                wordnet_openimage_label.write(label[0] + '\\t' + str(label[1]) + '\\t' + label_name_origin + '\\t'+ ','.join(label_synsets) + '\\n')\n",
    "                continue\n",
    "            \n",
    "            #不在WordNet内\n",
    "            num_out += 1\n",
    "#             print label[0] + '\\t' + str(label[1]) + '\\t' + label_name + '\\t'+ ''\n",
    "            wordnet_openimage_label.write(label[0] + '\\t' + str(label[1]) + '\\t' + label_name_origin + '\\t' + 'not exist' + '\\n')\n",
    "    except:\n",
    "        print str(label[0]) + ' error'\n",
    "        print str(label[1])\n",
    "        print label_name_origin\n",
    "        wordnet_openimage_label.write(label[0] + '\\t' + str(label[1]) + '\\t' + label_name_origin + '\\t' + ''+ '\\n')\n",
    "        num_error += 1\n",
    "wordnet_openimage_label.close()\n",
    "\n",
    "for wnid,des in des_dict.iteritems():\n",
    "    wordnet_openimage_description.write(wnid + '\\t' + des + '\\n')\n",
    "wordnet_openimage_description.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'haha'\n",
    "print str(num_in) + ' in WordNet' + '\\t' + str(num_out) + ' not in WordNet' + '\\t' + str(num_error) + ' error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不存在wordnet中的label，通过照片去找对应的synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordnet_openimage_label = codecs.open(\"./data/wordnet_openimage_label.tsv\", \"r\", \"utf-8\")\n",
    "for line in wordnet_openimage_label:\n",
    "    lbid, count, label, wnid = line.split('\\t')\n",
    "    wnid = wnid.strip('\\n')\n",
    "    if wnid == \"not exist\":\n",
    "        print label\n",
    "wordnet_openimage_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取imageID与labelName的对应字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imageID_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取machine_train_label文件,提取imageID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "machine_train_label = open(\"./data/labels.csv\", \"r\")\n",
    "for line in machine_train_label:\n",
    "    ImageID,Source,LabelName,Confidence = line.split(',')\n",
    "    if LabelName in imageID_dict:\n",
    "        imageID_dict[LabelName].add(ImageID)\n",
    "    else:\n",
    "        imageID_dict[LabelName] = set(ImageID)\n",
    "machine_train_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取machine_valid_label,提取imageID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "machine_valid_label = open(\"./data/labels_valid.csv\", \"r\")\n",
    "for line in machine_valid_label:\n",
    "    ImageID,Source,LabelName,Confidence = line.split(',')\n",
    "    if LabelName in imageID_dict:\n",
    "        imageID_dict[LabelName].add(ImageID)\n",
    "    else:\n",
    "        imageID_dict[LabelName] = set(ImageID)\n",
    "machine_valid_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取humman_valid_label文件，提取imageID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "humman_valid_label = open(\"./data/labels_humman_valid.csv\", \"r\")\n",
    "for line in humman_valid_label:\n",
    "    ImageID,Source,LabelName,Confidence = line.split(',')\n",
    "    if LabelName in imageID_dict:\n",
    "        imageID_dict[LabelName].add(ImageID)\n",
    "    else:\n",
    "        imageID_dict[LabelName] = set(ImageID)\n",
    "humman_valid_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取前2000label的imageID集合，并去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top3000_label_imageID size is 9016796\n"
     ]
    }
   ],
   "source": [
    "top3000_label_imageID = set()\n",
    "for label in top3000_label:\n",
    "    for imageID in imageID_dict[label[0]]:\n",
    "        top3000_label_imageID.add(imageID)\n",
    "#     top3000_label_imageID = top3000_label_imageID | imageID_dict[label[0]]\n",
    "print 'top3000_label_imageID size is ' + str(len(top3000_label_imageID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('print.v.01'), Synset('publish.v.02'), Synset('publish.v.03')]\n"
     ]
    }
   ],
   "source": [
    "print wordnet.synsets('publish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Lemma.name of Lemma('car.n.01.automobile')>\n",
      "<bound method Synset.lemma_names of Synset('car.n.01')>\n"
     ]
    }
   ],
   "source": [
    "print wordnet.lemma('car.n.01.automobile').name\n",
    "print wordnet.synset('car.n.01').lemma_names"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
